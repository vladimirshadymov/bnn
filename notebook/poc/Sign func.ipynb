{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # import main library\n",
    "from torch.autograd import Function # import Function to create custom activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._C as _C\n",
    "import torch.utils.hooks as hooks\n",
    "from torch._six import with_metaclass\n",
    "import functools\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class _ContextMethodMixin(object):\n",
    "\n",
    "    def save_for_backward(self, *tensors):\n",
    "        \"\"\"Saves given tensors for a future call to :func:`~Function.backward`.\n",
    "\n",
    "        **This should be called at most once, and only from inside the**\n",
    "        :func:`forward` **method.**\n",
    "\n",
    "        Later, saved tensors can be accessed through the :attr:`saved_tensors`\n",
    "        attribute; or, if the corresponding Variable is needed (e.g. for double\n",
    "        backwards), those can be accessed through the :attr:`saved_variables`\n",
    "        attribute.  Before returning them to the user, a check is made, to ensure\n",
    "        they weren't used in any in-place operation that modified their content.\n",
    "\n",
    "        Arguments can also be ``None``.\n",
    "        \"\"\"\n",
    "        self.to_save = tensors\n",
    "\n",
    "    def mark_dirty(self, *args):\n",
    "        \"\"\"Marks given tensors as modified in an in-place operation.\n",
    "\n",
    "        **This should be called at most once, only from inside the**\n",
    "        :func:`forward` **method, and all arguments should be inputs.**\n",
    "\n",
    "        Every tensor that's been modified in-place in a call to :func:`forward`\n",
    "        should be given to this function, to ensure correctness of our checks.\n",
    "        It doesn't matter whether the function is called before or after\n",
    "        modification.\n",
    "        \"\"\"\n",
    "        self.dirty_tensors = args\n",
    "\n",
    "    def mark_shared_storage(self, *pairs):\n",
    "        \"\"\"Marks that given pairs of distinct tensors are sharing storage.\n",
    "\n",
    "        **This should be called at most once, only from inside the**\n",
    "        :func:`forward` **method, and all arguments should be pairs of\n",
    "        (input, output).**\n",
    "\n",
    "        If some of the outputs are going to be tensors sharing storage with\n",
    "        some of the inputs, all pairs of (input_arg, output_arg) should be\n",
    "        given to this function, to ensure correctness checking of in-place\n",
    "        modification. The only exception is when an output is exactly the same\n",
    "        tensor as input (e.g. in-place ops). In such case it's easy to conclude\n",
    "        that they're sharing data, so we don't require specifying such\n",
    "        dependencies.\n",
    "\n",
    "        This function is not needed in most functions. It's primarily used in\n",
    "        indexing and transpose ops.\n",
    "        \"\"\"\n",
    "        self.shared_pairs = pairs\n",
    "\n",
    "    def mark_non_differentiable(self, *args):\n",
    "        \"\"\"Marks outputs as non-differentiable.\n",
    "\n",
    "        **This should be called at most once, only from inside the**\n",
    "        :func:`forward` **method, and all arguments should be outputs.**\n",
    "\n",
    "        This will mark outputs as not requiring gradients, increasing the\n",
    "        efficiency of backward computation. You still need to accept a gradient\n",
    "        for each output in :meth:`~Function.backward`, but it's always going to\n",
    "        be ``None``.\n",
    "\n",
    "        This is used e.g. for indices returned from a max :class:`Function`.\n",
    "        \"\"\"\n",
    "        self.non_differentiable = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignEst(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "\n",
    "        ctx.save_for_backward(input)  # save input for backward pass\n",
    "\n",
    "        # clone the input tensor\n",
    "        output = input.clone()\n",
    "        output[output >= 0] = 1.\n",
    "        output[output < 0] = -1.\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = None  # set output to None\n",
    "        #input, = ctx.saved_tensors\n",
    "        input = torch.tensor([0.7, -1.2, 0., 2.3])\n",
    "        grad_input = input.clone()\n",
    "        grad_input[torch.abs(input)>=1.] = 0.\n",
    "        grad_input[torch.abs(input)<1.] = 1.\n",
    "        grad_input = grad_input*grad_output\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.7, -1.2, 0., 2.3])\n",
    "ctx = _ContextMethodMixin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1., -1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "b = SignEst.forward(ctx, a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.0000, 2.0000, 0.0000])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_output = torch.tensor([0.5, 1.5, 2., 3.])\n",
    "grad_input = SignEst.backward(ctx, grad_output)\n",
    "grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
